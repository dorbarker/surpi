#!/usr/bin/env python3

import argparse
import subprocess
import tempfile
from functools import partial
from pathlib import Path
from multiprocessing import cpu_count
from utilities import logtime, annotated_to_fastq, fastq_to_fasta, sam_to_fasta
from Bio import SeqIO
from taxonomy_lookup import taxonomy_lookup, table_generator
from preprocessing import crop_reads
import update_sam

def arguments():

    parser = argparse.ArgumentParser()

    parser.add_argument('--sample', type=Path,
                        help='Input fastq file')

    parser.add_argument('--workdir', type=Path,
                        help='Work directory for SURPI')

    parser.add_argument_group('--cores', type=int,
                              default=cpu_count(),
                              help='CPU cores to use [all]')

    return parser.parse_args()

@logtime('Separate matched and unmatched sequences from SAM file')
def separate_sam_lines(sam_file):
    '''Separates SAM-formatted file.

    Lines where the third column is '*' are appended to matched,
    and appended to unmatched otherwise.
    '''

    matched, unmatched = [], []

    with open(sam_file, 'r') as sam:

        for line in sam:
            if not line.startswith('@'):

                split_line = line.split('\t')

                if split_line[2] == '*':
                    matched.append(line)
                else:
                    unmatched.append(line)

    return matched, unmatched

def write_sam(lines, outpath):
    '''Writes SAM lines generated by separate_sam_lines()'''

    with open(outpath, 'w') as out:

        out.write('\n'.join(lines))

@logtime('SNAP unmatched sequences')
def snap_unmatched(infile, workdir, tempdir, snap_db_dir, edit_distance, cores):

    first_pass_done = False


    with tempfile.TemporaryDirectory(dir=tempdir) as ephemeral:

        throw_away = Path(ephemeral)

        tmp_fastq = (throw_away / infile.stem).with_suffix('.tmp.fastq')
        tmp_sam = tmp_fastq.with_suffix('.sam')
        prev_sam = tmp_sam.with_suffix('.prev')

        for snap_db in snap_db_dir.glob('*'):

            if not first_pass_done:

                snap_cmd = ('snap-aligner', 'single', snap_db, infile,
                            '-o', tmp_sam, '-t', cores, '-x', '-f', '-h', 250,
                            '-d', edit_distance, '-n', 25)

            else:

                snap_cmd = ('snap-aligner', 'single', snap_db, tmp_fastq,
                            '-o', tmp_sam, '-t', cores, '-x', '-f', '-h', 250,
                            '-d', edit_distance, '-n', 25)

            subprocess.check_call(map(str, snap_cmd))

            update_sam.compare_sam(tmp_sam, prev_sam)
            tmp_fastq.write_text(annotated_to_fastq(prev_sam))

            first_pass_done = True

        # *.NT.sam doesn't appear anywhere else in the original SURPI - WTF?
            # Using *.NT.snap.sam instead
        update_sam.update_sam(prev_sam,
                              workdir / infile.with_suffix('.NT.snap.sam'))

def separate_ranked_taxonomy(ranks, annotations, result_dir):
    '''Recursively searches through `ranks` and writes taxonomy results
    to files name after the given rank.

    Ranks must be ordered from broad to narrow,
    e.g. ['Eurkaryota', 'Chordata', 'Mammalia', 'Primates']
    '''

    def condition(line, to_find, to_exclude):
        '''Returns True if the `to_find` pattern is in `line` and
        `to_exclude` is not in `line`
        '''
        return to_find in line and to_exclude not in line

    def tax_search_recursor(rank_list):

        to_find, to_exclude = rank_list[-2:]

        test = partial(condition, to_find=to_find, to_exclude=to_exclude)

        result_file = (result_dir / to_find).with_suffix('.annotated')

        sub_annot = (line for line in annotations if test(line))

        result_file.write_text('\n'.join(sub_annot))

        if len(rank_list) > 2:

            rank_list = rank_list[:-1]

            tax_search_recursor(rank_list)

        else:

            return result_file

    ranks_ = ranks.append('loremipsumdolorsitamet')  # ensure never found

    result_file = tax_search_recursor(ranks_)

    return result_file

def ribo_snap(inputfile, mode, cores, ribo_dir, tempdir):
    '''Runs SNAP to subtract `inputfile` reads from databases
    of bacterial and ribosomal reads
    '''

    def extract_sam_from_sam(infile, outfile, riboremoved):
        '''Writes to `outfile` the reads in `infile` whose headers
        are found in `riboremoved`
        '''

        with riboremoved.open('r') as noribo:
            headers = [line.split()[0] for line in noribo]

        with infile.open('r') as inf, outfile.open('w') as output:

            for line in inf:

                if line.split()[0] in headers:
                    output.write(line)

    noribo = inputfile.with_suffix('.noribo.annotated')

    if mode == 'BAC':

        snap_large = ribo_dir / 'snap_index_23sRNA'
        snap_small = ribo_dir / 'snap_index_rdp_typed_iso_goodq_9210seqs'

    else:

        snap_large = ribo_dir / \
                'snap_index_28s_rRNA_gene_NOT_partial_18s_spacer_5.8s.fa'
        snap_small = ribo_dir / 'snap_index_18s_rRNA_gene_not_partial.fa'

    with tempfile.TemporaryDirectory(dir=str(tempdir)) as ephemeral:

        throw_away = Path(ephemeral)
        fasta = (throw_away / inputfile.stem).with_suffix('.fasta')
        fastq = fasta.with_suffix('.fastq')
        crop = fastq.with_suffix('.fastq.crop')
        large_out = crop.with_suffix('.noLargeS.unmatched')
        small_out = crop.with_suffix('.noSmallS_LargeS.sam')

        sam_to_fasta(inputfile, fasta)

        fastq.write_text(subprocess.check_output(('fasta_to_fastq', str(fasta)),
                                                 universal_newlines=True))

        crop_reads(fastq, crop, 10, 75)

        snap1 = ('snap', 'single', snap_large, crop, '-o', large_out,
                 '-t', cores, '-f', '-h', 250, '-d', 18, '-n', 200, '-F', 'u')

        subprocess.check_call(map(str, snap1))

        # inplace conversion of sam -> fastq
        large_out.write_text(annotated_to_fastq(large_out))

        snap2 = ('snap', 'single', snap_small, large_out, '-o', small_out,
                 '-t', cores, '-h', 250, '-d', 18, '-n', 200, '-F', 'u')

        subprocess.check_call(map(str, snap2))

        extract_sam_from_sam(inputfile, noribo, small_out)

    table_generator(noribo, 'SNAP', 'N', 'Y', 'N', 'N')

def extract_headers(basef, workdir, cores):

    cmd = (
        'extractHeaderFromFastq_ncores.sh', cores,
        workdir / basef.with_suffix('.cutadapt.fastq'),
        workdir / basef.with_suffix('.NT.snap.matched.sam'),
        workdir / basef.with_suffix('.NT.snap.matched.fulllength.fastq'),
        workdir / basef.with_suffix('.NT.snap.unmatched.sam'),
        workdir / basef.with_suffix('.NT.snap.unmatched.fulllength.fastq')
        )

    subprocess.check_call(map(str, cmd))

    return cmd[-1]

def lookup_taxonomy(basef, annotated, workdir, tax_db_dir):
    '''Looks up the taxonomy of annotated reads'''

    full_length_fastq = basef.with_suffix('.NT.snap.matched.fulllength.fastq')
    full_length_sam = full_length_fastq.with_suffix('.sam')

    # sort sam
    with open(workdir / basef.with_suffix('.NT.snap.matched.sam')) as sam:

        lines = sam.readlines()
        sorted_lines = sorted(lines, key=lambda x: x[0])

    # may need to double-check sequence sorting here
    with open(workdir / full_length_fastq) as fastq:
        seq_quals = []
        for rec in SeqIO.parse(fastq, 'fastq'):
            fastq_lines = rec.splitlines()
            seq_quals.append([fastq_lines[1], fastq_lines[3]])

    # paste
    with open(full_length_sam, 'w') as out_sam:

        for line, qual  in zip(sorted_lines, seq_quals):

            to_write = line[:9] + qual + line[11:] + ['\n']

            out_sam.write('\t'.join(to_write))

    taxonomy_lookup(infile=full_length_sam,
                    outfile=annotated,
                    db_dir=tax_db_dir,
                    file_type='sam',
                    seq_type='nucl')

    # TODO: check final annotation sort -k 13.7n

    viruses = separate_ranked_taxonomy(ranks=('Viruses',),
                                       annotations=annotated,
                                       result_dir=workdir)

    bacteria = separate_ranked_taxonomy(ranks=('Bacteria',),
                                        annotations=annotated,
                                        result_dir=workdir)

    euks = separate_ranked_taxonomy(ranks=('Eukaryota', 'Chordata',
                                           'Mammalia', 'Primates'),
                                    annotations=annotated,
                                    result_dir=workdir)

    return viruses, bacteria, euks

def extract_to_fast(fastq, fasta, output, tempdir):

    def subseq(infile, parent, outfile):

        header_file = infile.with_suffix('.headers')
        headers = [line[1:] for line in infile.read_text().splitlines()
                   if line.startswith('>')]

        header_file.write_text('\n'.join(headers))

        cmd = ('seqtk', 'subseq', parent, header_file)

        output = subprocess.check_output(cmd)

        Path(output).write_text(output)

        header_file.unlink()

    def sequniq(infile, outfile):
        '''Filters redundant sequence from `infile`'''

        cmd = ('gt', 'sequniq', '-seqit', '-force', '-o', outfile, infile)

        subprocess.check_call(map(str, cmd))

    def sort_fasta_by_length(unsorted_fasta, sorted_fasta):
        '''Sorts a FASTA file by descending sequence length and writes it to
        a new FASTA file
        '''

        with unsorted_fasta.open('r') as fasta, sorted_fasta.open('w') as out:

            records = SeqIO.parse(fasta, 'fasta')

            sorted_records = sorted(records,
                                    key=lambda x: len(x.seq),
                                    reverse=True)

            SeqIO.write(sorted_records, out, 'fasta')

    fastq_to_fasta(fastq, fasta)

    with tempfile.TemporaryDirectory(dir=str(tempdir)) as ephemeral:

        temp_dir_path = Path(ephemeral)

        sorted_fasta = (temp_dir_path / fasta).with_suffix('.sorted')

        cropped = sorted_fasta.with_suffix('.cropped')

        uniq = cropped.with_suffix('.uniq')

        sort_fasta_by_length(fasta, sorted_fasta)

        crop_reads(sorted_fasta, cropped, 25, 50)

        sequniq(cropped, uniq)

        subseq(uniq, fasta, output)

def snap(sample, workdir, snap_db_dir, tax_db_dir, ribo_dir, cores,
         edit_distance, cache_reset, comprehensive, temp_dir):

    basef = Path(sample.stem)
    annotated = workdir / basef.with_suffix('.annotated')

    snap_unmatched(basef, workdir, snap_db_dir, unmatched, cache_reset,
                   edit_distance, cores)

    snap_sam = workdir / basef.with_suffix('.NT.snap.sam')

    matched, unmatched = separate_sam_lines(snap_sam)

    write_sam(matched, workdir / basef.with_suffix('.NT.snap.matched.sam'))
    write_sam(unmatched, workdir / basef.with_suffix('.NT.snap.unmatched.sam'))

    if not annotated.exists():

        fulllength_fastq = extract_headers(basef, workdir, cores)
        viruses, bacteria, euks = lookup_taxonomy(basef, annotated, workdir,
                                                  tax_db_dir, ribo_dir, cores)

        viruses_fastq = viruses.with_suffix('.fastq')

    ribo_snap(bacteria, 'BAC', cores, ribo_dir, temp_dir)
    ribo_snap(euks, 'EUK', cores, ribo_dir, temp_dir)

    table_generator(viruses, 'SNAP', 'Y', 'Y', 'Y', 'Y')

    if comprehensive:

        viruses_fastq.write_text(annotated_to_fastq(viruses))

        # output of this given to denovo assembly and RAPSearch
        extract_to_fast(fulllength_fastq,
                        fulllength_fastq.with_suffix('.fasta'),
                        fulllength_fastq.with_suffix('.uniq.f1.fasta'))

    return viruses, viruses_fastq, fulllength_fastq.with_suffix('.uniq.f1.fasta')

def main():

    args = arguments()

    #snap(args.sample, args.workdir, args.db_dir, args.cores)

if __name__ == '__main__':
    main()
